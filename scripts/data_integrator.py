#!/usr/bin/env python3
"""
Enhanced Five-Step MoE (Mixture of Experts) Workflow - Deep Analysis Edition
æ·±åº¦ç ”åˆ¤å‡çº§ç‰ˆ - æ•´åˆæŠ€æœ¯é¢ã€åŸºæœ¬é¢ã€æƒ…æŠ¥é¢ã€èµ„é‡‘é¢å››ç»´åˆ†æ

æ ¸å¿ƒå‡çº§:
1. å¤šç»´åº¦æ•°æ®æ•´åˆ (æŠ€æœ¯/åŸºæœ¬é¢/æƒ…æŠ¥/èµ„é‡‘)
2. å†å²ç­–ç•¥å¤ç›˜å­¦ä¹ 
3. æƒ…æŠ¥æƒé‡åŠ¨æ€è¯„ä¼°
4. ä¸»åŠ›æ„å›¾æ·±åº¦è¯†åˆ«
5. å¸‚åœºæƒ…ç»ªé‡åŒ–åˆ†æ
"""

import json
import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path

# Import existing AI functions
from utils.ai_advisor import (
    call_deepseek_api,
    call_qwen_api,
    build_advisor_prompt,
    build_red_team_prompt,
    build_refinement_prompt,
    build_final_decision_prompt
)
from utils.prompt_loader import load_all_prompts


class DataIntegrator:
    """æ•°æ®æ•´åˆå™¨ - èšåˆæ‰€æœ‰å¯ç”¨æ•°æ®æº"""
    
    def __init__(self, symbol: str, base_path: str = '/Users/zuliangzhao/MarketMonitorAndBuyer'):
        self.symbol = symbol
        self.base_path = Path(base_path)
        self.stock_data_path = self.base_path / 'stock_data'
        
    def load_all_data(self) -> Dict[str, Any]:
        """åŠ è½½æ‰€æœ‰å¯ç”¨æ•°æ®"""
        data = {
            'symbol': self.symbol,
            'timestamp': datetime.now().isoformat(),
            'technical': self._load_technical_data(),
            'fundamental': self._load_fundamental_data(),
            'fund_flow': self._load_fund_flow_data(),
            'intelligence': self._load_intelligence_data(),
            'research_history': self._load_research_history(),
            'strategy_history': self._load_strategy_history(),
            'minute_data': self._load_minute_data(),
            'market_sentiment': self._analyze_market_sentiment()
        }
        return data
    
    def _load_technical_data(self) -> Dict:
        """åŠ è½½æŠ€æœ¯æŒ‡æ ‡æ•°æ®"""
        try:
            # ä»åˆ†é’Ÿæ•°æ®è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            minute_file = self.stock_data_path / f'{self.symbol}_minute.parquet'
            if minute_file.exists():
                df = pd.read_parquet(minute_file)
                if len(df) > 0:
                    recent = df.tail(240)  # æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥
                    return {
                        'current_price': float(df['æ”¶ç›˜'].iloc[-1]) if 'æ”¶ç›˜' in df.columns else None,
                        'price_change_1d': self._calc_price_change(df, 240),
                        'price_change_5d': self._calc_price_change(df, 240*5),
                        'volatility': float(df['æ”¶ç›˜'].pct_change().std() * 100) if 'æ”¶ç›˜' in df.columns else None,
                        'avg_volume': int(recent['æˆäº¤é‡'].mean()) if 'æˆäº¤é‡' in recent.columns else 0,
                        'volume_trend': self._analyze_volume_trend(df),
                        'support_level': self._calc_support_level(df),
                        'resistance_level': self._calc_resistance_level(df),
                        'data_available': True
                    }
        except Exception as e:
            print(f"âš ï¸ æŠ€æœ¯æŒ‡æ ‡åŠ è½½å¤±è´¥: {e}")
        return {'data_available': False}
    
    def _load_fundamental_data(self) -> Dict:
        """åŠ è½½åŸºæœ¬é¢æ•°æ®"""
        fundamental = {'data_available': False}
        try:
            research_file = self.stock_data_path / f'{self.symbol}_research.json'
            if research_file.exists():
                with open(research_file, 'r', encoding='utf-8') as f:
                    research_list = json.load(f)
                    if research_list:
                        # æå–æœ€æ–°çš„åŸºæœ¬é¢ä¿¡æ¯
                        latest = research_list[-1]
                        content = latest.get('result', '')
                        
                        # è§£æåŸºæœ¬é¢å…³é”®ä¿¡æ¯
                        fundamental = {
                            'data_available': True,
                            'latest_research_date': latest.get('timestamp', 'N/A'),
                            'research_count': len(research_list),
                            'financial_summary': self._extract_financial_info(content),
                            'key_news': self._extract_key_news(content),
                            'risk_factors': self._extract_risk_factors(content),
                            'catalysts': self._extract_catalysts(content)
                        }
        except Exception as e:
            print(f"âš ï¸ åŸºæœ¬é¢æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return fundamental
    
    def _load_fund_flow_data(self) -> Dict:
        """åŠ è½½èµ„é‡‘æµå‘æ•°æ®"""
        try:
            fund_flow_file = self.stock_data_path / 'fund_flow_cache.parquet'
            if fund_flow_file.exists():
                df = pd.read_parquet(fund_flow_file)
                symbol_data = df[df['symbol'] == self.symbol]
                if len(symbol_data) > 0:
                    latest = symbol_data.iloc[-1]
                    return {
                        'data_available': True,
                        'main_force_net': latest.get('ä¸»åŠ›å‡€æµå…¥', 0),
                        'main_force_ratio': latest.get('ä¸»åŠ›å‡€å æ¯”', 0),
                        'super_large_net': latest.get('è¶…å¤§å•å‡€æµå…¥', 0),
                        'large_net': latest.get('å¤§å•å‡€æµå…¥', 0),
                        'medium_net': latest.get('ä¸­å•å‡€æµå…¥', 0),
                        'small_net': latest.get('å°å•å‡€æµå…¥', 0),
                        '5day_trend': self._calc_fund_flow_trend(df, self.symbol, 5),
                        '10day_trend': self._calc_fund_flow_trend(df, self.symbol, 10),
                        'main_intent': self._analyze_main_intent(df, self.symbol)
                    }
        except Exception as e:
            print(f"âš ï¸ èµ„é‡‘æµå‘åŠ è½½å¤±è´¥: {e}")
        return {'data_available': False}
    
    def _load_intelligence_data(self) -> Dict:
        """åŠ è½½æƒ…æŠ¥åº“æ•°æ®"""
        intel = {'data_available': False, 'items': []}
        try:
            db_path = self.stock_data_path / 'intel_hub.db'
            if db_path.exists():
                conn = sqlite3.connect(str(db_path))
                cursor = conn.cursor()
                
                # è·å–æ´»è·ƒæƒ…æŠ¥
                cursor.execute("""
                    SELECT content, source, priority, marked_by_user, created_at 
                    FROM intelligence 
                    WHERE symbol = ? AND is_active = 1
                    ORDER BY marked_by_user DESC, priority DESC, created_at DESC
                    LIMIT 10
                """, (self.symbol,))
                
                rows = cursor.fetchall()
                intel['items'] = [{
                    'content': row[0],
                    'source': row[1],
                    'priority': row[2],
                    'marked': bool(row[3]),
                    'date': row[4]
                } for row in rows]
                
                # è·å–æ–°é—»ç¼“å­˜
                cursor.execute("""
                    SELECT title, content, source, published_at 
                    FROM news_cache 
                    WHERE symbol = ? 
                    ORDER BY published_at DESC 
                    LIMIT 5
                """, (self.symbol,))
                
                news_rows = cursor.fetchall()
                intel['recent_news'] = [{
                    'title': row[0],
                    'content': row[1],
                    'source': row[2],
                    'date': row[3]
                } for row in news_rows]
                
                intel['data_available'] = len(intel['items']) > 0 or len(intel.get('recent_news', [])) > 0
                conn.close()
        except Exception as e:
            print(f"âš ï¸ æƒ…æŠ¥åº“åŠ è½½å¤±è´¥: {e}")
        return intel
    
    def _load_research_history(self) -> List[Dict]:
        """åŠ è½½å†å²ç ”ç©¶æŠ¥å‘Š"""
        try:
            research_file = self.stock_data_path / f'{self.symbol}_research.json'
            if research_file.exists():
                with open(research_file, 'r', encoding='utf-8') as f:
                    return json.load(f)[-5:]  # æœ€è¿‘5ä»½æŠ¥å‘Š
        except Exception as e:
            print(f"âš ï¸ å†å²ç ”æŠ¥åŠ è½½å¤±è´¥: {e}")
        return []
    
    def _load_strategy_history(self) -> List[Dict]:
        """åŠ è½½å†å²ç­–ç•¥è®°å½•"""
        try:
            strategy_file = self.stock_data_path / f'{self.symbol}_strategies.json'
            if strategy_file.exists():
                with open(strategy_file, 'r', encoding='utf-8') as f:
                    strategies = json.load(f)
                    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æ—¶é—´æ’åº
                    return [{'date': k, **v} for k, v in strategies.items()][-5:]
        except Exception as e:
            print(f"âš ï¸ å†å²ç­–ç•¥åŠ è½½å¤±è´¥: {e}")
        return []
    
    def _load_minute_data(self) -> Dict:
        """åŠ è½½åˆ†é’Ÿæ•°æ®æ‘˜è¦"""
        try:
            minute_file = self.stock_data_path / f'{self.symbol}_minute.parquet'
            if minute_file.exists():
                df = pd.read_parquet(minute_file)
                if len(df) > 0:
                    recent_5d = df.tail(240*5)
                    return {
                        'data_available': True,
                        'total_records': len(df),
                        'recent_5d_high': float(recent_5d['æœ€é«˜'].max()) if 'æœ€é«˜' in recent_5d.columns else None,
                        'recent_5d_low': float(recent_5d['æœ€ä½'].min()) if 'æœ€ä½' in recent_5d.columns else None,
                        'price_distribution': {
                            'q25': float(recent_5d['æ”¶ç›˜'].quantile(0.25)) if 'æ”¶ç›˜' in recent_5d.columns else None,
                            'q75': float(recent_5d['æ”¶ç›˜'].quantile(0.75)) if 'æ”¶ç›˜' in recent_5d.columns else None
                        }
                    }
        except Exception as e:
            print(f"âš ï¸ åˆ†é’Ÿæ•°æ®åŠ è½½å¤±è´¥: {e}")
        return {'data_available': False}
    
    def _analyze_market_sentiment(self) -> Dict:
        """åˆ†æå¸‚åœºæƒ…ç»ª"""
        sentiment = {'data_available': False}
        try:
            # åŸºäºèµ„é‡‘æµå‘å’Œä»·æ ¼è¡Œä¸ºåˆ†ææƒ…ç»ª
            fund_flow = self._load_fund_flow_data()
            technical = self._load_technical_data()
            
            if fund_flow.get('data_available') and technical.get('data_available'):
                # è®¡ç®—æƒ…ç»ªæŒ‡æ ‡
                main_ratio = fund_flow.get('main_force_ratio', 0)
                price_change = technical.get('price_change_1d', 0)
                
                # ç»¼åˆåˆ¤æ–­
                if main_ratio > 10 and price_change > 2:
                    sentiment['overall'] = 'å¼ºçƒˆä¹è§‚'
                elif main_ratio > 5:
                    sentiment['overall'] = 'ä¹è§‚'
                elif main_ratio < -10 and price_change < -2:
                    sentiment['overall'] = 'å¼ºçƒˆæ‚²è§‚'
                elif main_ratio < -5:
                    sentiment['overall'] = 'æ‚²è§‚'
                else:
                    sentiment['overall'] = 'ä¸­æ€§'
                
                sentiment['main_force_attitude'] = 'çœ‹å¤š' if main_ratio > 5 else 'çœ‹ç©º' if main_ratio < -5 else 'è§‚æœ›'
                sentiment['retail_attitude'] = 'è·Ÿé£' if fund_flow.get('small_net', 0) > 0 else 'ææ…Œ'
                sentiment['data_available'] = True
        except Exception as e:
            print(f"âš ï¸ æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
        return sentiment
    
    # Helper methods
    def _calc_price_change(self, df: pd.DataFrame, periods: int) -> float:
        """è®¡ç®—ä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”"""
        try:
            if 'æ”¶ç›˜' in df.columns and len(df) >= periods:
                return float((df['æ”¶ç›˜'].iloc[-1] / df['æ”¶ç›˜'].iloc[-periods] - 1) * 100)
        except:
            pass
        return 0.0
    
    def _analyze_volume_trend(self, df: pd.DataFrame) -> str:
        """åˆ†ææˆäº¤é‡è¶‹åŠ¿"""
        try:
            if 'æˆäº¤é‡' in df.columns and len(df) > 480:
                recent = df['æˆäº¤é‡'].tail(240).mean()
                previous = df['æˆäº¤é‡'].tail(480).head(240).mean()
                ratio = recent / previous if previous > 0 else 1
                if ratio > 1.5:
                    return 'æ”¾é‡'
                elif ratio < 0.7:
                    return 'ç¼©é‡'
                return 'æŒå¹³'
        except:
            pass
        return 'æœªçŸ¥'
    
    def _calc_support_level(self, df: pd.DataFrame) -> float:
        """è®¡ç®—æ”¯æ’‘ä½"""
        try:
            if 'æœ€ä½' in df.columns:
                return float(df['æœ€ä½'].tail(120).min())
        except:
            pass
        return 0.0
    
    def _calc_resistance_level(self, df: pd.DataFrame) -> float:
        """è®¡ç®—é˜»åŠ›ä½"""
        try:
            if 'æœ€é«˜' in df.columns:
                return float(df['æœ€é«˜'].tail(120).max())
        except:
            pass
        return 0.0
    
    def _extract_financial_info(self, content: str) -> Dict:
        """ä»ç ”æŠ¥ä¸­æå–è´¢åŠ¡ä¿¡æ¯"""
        info = {}
        # ç®€å•çš„å…³é”®è¯æå–
        if 'è¥æ”¶' in content or 'æ”¶å…¥' in content:
            info['has_revenue_data'] = True
        if 'å‡€åˆ©æ¶¦' in content or 'äºæŸ' in content:
            info['has_profit_data'] = True
        if 'æ¯›åˆ©ç‡' in content:
            info['has_margin_data'] = True
        return info
    
    def _extract_key_news(self, content: str) -> List[str]:
        """æå–å…³é”®æ–°é—»"""
        news = []
        lines = content.split('\n')
        for line in lines:
            if any(keyword in line for keyword in ['å…¬å‘Š', 'æ–°é—»', 'æ¶ˆæ¯', 'å‘å¸ƒ', 'è®¡åˆ’']):
                if len(line) > 10 and len(line) < 200:
                    news.append(line.strip())
        return news[:5]
    
    def _extract_risk_factors(self, content: str) -> List[str]:
        """æå–é£é™©å› ç´ """
        risks = []
        risk_keywords = ['é£é™©', 'äºæŸ', 'è´Ÿå€º', 'è¯‰è®¼', 'å¤„ç½š', 'é€€å¸‚', 'è¿çº¦', 'æ‹…ä¿']
        lines = content.split('\n')
        for line in lines:
            if any(kw in line for kw in risk_keywords):
                if len(line) > 10 and len(line) < 200:
                    risks.append(line.strip())
        return risks[:5]
    
    def _extract_catalysts(self, content: str) -> List[str]:
        """æå–å‚¬åŒ–å‰‚"""
        catalysts = []
        catalyst_keywords = ['å¹¶è´­', 'é‡ç»„', 'å¢æŒ', 'å›è´­', 'è‚¡æƒæ¿€åŠ±', 'æ–°äº§å“', 'è®¢å•', 'åˆä½œ']
        lines = content.split('\n')
        for line in lines:
            if any(kw in line for kw in catalyst_keywords):
                if len(line) > 10 and len(line) < 200:
                    catalysts.append(line.strip())
        return catalysts[:5]
    
    def _calc_fund_flow_trend(self, df: pd.DataFrame, symbol: str, days: int) -> Dict:
        """è®¡ç®—èµ„é‡‘æµå‘è¶‹åŠ¿"""
        try:
            symbol_data = df[df['symbol'] == symbol].tail(days)
            if len(symbol_data) > 0:
                return {
                    'total_net': float(symbol_data['ä¸»åŠ›å‡€æµå…¥'].sum()),
                    'positive_days': int((symbol_data['ä¸»åŠ›å‡€æµå…¥'] > 0).sum()),
                    'avg_daily': float(symbol_data['ä¸»åŠ›å‡€æµå…¥'].mean())
                }
        except:
            pass
        return {'total_net': 0, 'positive_days': 0, 'avg_daily': 0}
    
    def _analyze_main_intent(self, df: pd.DataFrame, symbol: str) -> str:
        """åˆ†æä¸»åŠ›æ„å›¾"""
        try:
            symbol_data = df[df['symbol'] == symbol].tail(10)
            if len(symbol_data) < 5:
                return 'æ•°æ®ä¸è¶³'
            
            recent_net = symbol_data['ä¸»åŠ›å‡€æµå…¥'].tail(3).sum()
            price_trend = symbol_data['close'].iloc[-1] / symbol_data['close'].iloc[0] - 1 if 'close' in symbol_data.columns else 0
            
            if recent_net > 5000 and price_trend > 0.05:
                return 'ç§¯æå»ºä»“'
            elif recent_net > 5000 and price_trend < 0:
                return 'é€†åŠ¿å¸ç­¹'
            elif recent_net < -5000 and price_trend > 0:
                return 'æ‹‰é«˜å‡ºè´§'
            elif recent_net < -5000 and price_trend < 0:
                return 'ææ…ŒæŠ›å”®'
            else:
                return 'éœ‡è¡æ•´ç†'
        except:
            pass
        return 'æœªçŸ¥'


def format_enriched_context(data: Dict[str, Any]) -> str:
    """å°†æ•´åˆçš„æ•°æ®æ ¼å¼åŒ–ä¸ºAIå¯è¯»çš„æ–‡æœ¬"""
    lines = []
    lines.append("=" * 60)
    lines.append("ã€ğŸ” æ·±åº¦å¤šç»´æ•°æ®æ•´åˆæŠ¥å‘Šã€‘")
    lines.append("=" * 60)
    
    # 1. æŠ€æœ¯é¢åˆ†æ
    tech = data.get('technical', {})
    if tech.get('data_available'):
        lines.append("\nğŸ“Š ã€æŠ€æœ¯é¢åˆ†æã€‘")
        lines.append(f"   å½“å‰ä»·æ ¼: {tech.get('current_price', 'N/A')}")
        lines.append(f"   1æ—¥æ¶¨è·Œ: {tech.get('price_change_1d', 0):+.2f}%")
        lines.append(f"   5æ—¥æ¶¨è·Œ: {tech.get('price_change_5d', 0):+.2f}%")
        lines.append(f"   æ³¢åŠ¨ç‡: {tech.get('volatility', 0):.2f}%")
        lines.append(f"   æˆäº¤é‡è¶‹åŠ¿: {tech.get('volume_trend', 'N/A')}")
        lines.append(f"   æ”¯æ’‘ä½: {tech.get('support_level', 'N/A')}")
        lines.append(f"   é˜»åŠ›ä½: {tech.get('resistance_level', 'N/A')}")
    
    # 2. èµ„é‡‘é¢åˆ†æ
    fund = data.get('fund_flow', {})
    if fund.get('data_available'):
        lines.append("\nğŸ’° ã€èµ„é‡‘é¢åˆ†æã€‘")
        lines.append(f"   ä¸»åŠ›å‡€æµå…¥: {fund.get('main_force_net', 0):.0f}ä¸‡")
        lines.append(f"   ä¸»åŠ›å‡€å æ¯”: {fund.get('main_force_ratio', 0):.2f}%")
        lines.append(f"   è¶…å¤§å•æµå‘: {fund.get('super_large_net', 0):.0f}ä¸‡")
        lines.append(f"   5æ—¥èµ„é‡‘æµå‘: {fund.get('5day_trend', {}).get('total_net', 0):.0f}ä¸‡")
        lines.append(f"   ä¸»åŠ›æ„å›¾åˆ¤æ–­: {fund.get('main_intent', 'æœªçŸ¥')}")
    
    # 3. åŸºæœ¬é¢åˆ†æ
    fundamental = data.get('fundamental', {})
    if fundamental.get('data_available'):
        lines.append("\nğŸ“ˆ ã€åŸºæœ¬é¢åˆ†æã€‘")
        lines.append(f"   ç ”æŠ¥æ•°é‡: {fundamental.get('research_count', 0)}ä»½")
        lines.append(f"   æœ€æ–°ç ”æŠ¥æ—¥æœŸ: {fundamental.get('latest_research_date', 'N/A')}")
        
        risks = fundamental.get('risk_factors', [])
        if risks:
            lines.append(f"   âš ï¸ ä¸»è¦é£é™©å› ç´  ({len(risks)}é¡¹):")
            for risk in risks[:3]:
                lines.append(f"      â€¢ {risk[:80]}...")
        
        catalysts = fundamental.get('catalysts', [])
        if catalysts:
            lines.append(f"   ğŸš€ æ½œåœ¨å‚¬åŒ–å‰‚ ({len(catalysts)}é¡¹):")
            for cat in catalysts[:3]:
                lines.append(f"      â€¢ {cat[:80]}...")
    
    # 4. å¸‚åœºæƒ…ç»ª
    sentiment = data.get('market_sentiment', {})
    if sentiment.get('data_available'):
        lines.append("\nğŸ­ ã€å¸‚åœºæƒ…ç»ªåˆ†æã€‘")
        lines.append(f"   æ•´ä½“æƒ…ç»ª: {sentiment.get('overall', 'N/A')}")
        lines.append(f"   ä¸»åŠ›æ€åº¦: {sentiment.get('main_force_attitude', 'N/A')}")
        lines.append(f"   æ•£æˆ·æƒ…ç»ª: {sentiment.get('retail_attitude', 'N/A')}")
    
    # 5. æƒ…æŠ¥åº“
    intel = data.get('intelligence', {})
    if intel.get('data_available'):
        lines.append(f"\nğŸ“š ã€æƒ…æŠ¥åº“ã€‘ ({len(intel.get('items', []))}æ¡æ ¸å¿ƒæƒ…æŠ¥)")
        for item in intel.get('items', [])[:3]:
            prefix = "â­" if item.get('marked') else "â€¢"
            lines.append(f"   {prefix} [{item.get('priority', 'normal')}] {item.get('content', '')[:100]}...")
    
    # 6. å†å²ç­–ç•¥å¤ç›˜
    strategy_history = data.get('strategy_history', [])
    if strategy_history:
        lines.append(f"\nğŸ“œ ã€å†å²ç­–ç•¥å¤ç›˜ã€‘ (æœ€è¿‘{len(strategy_history)}æ¬¡)")
        for i, strat in enumerate(strategy_history[-3:], 1):
            date = strat.get('date', 'N/A')
            advice = strat.get('advice', '')
            # æå–å†³ç­–æ–¹å‘
            direction = 'æœªçŸ¥'
            if 'å–å‡º' in advice[:500]:
                direction = 'å–å‡º'
            elif 'ä¹°å…¥' in advice[:500]:
                direction = 'ä¹°å…¥'
            elif 'è§‚æœ›' in advice[:500]:
                direction = 'è§‚æœ›'
            lines.append(f"   {i}. {date}: {direction}")
    
    lines.append("\n" + "=" * 60)
    return "\n".join(lines)


# Export for use in five_step_moe
__all__ = ['DataIntegrator', 'format_enriched_context']


if __name__ == "__main__":
    # Test
    integrator = DataIntegrator('600076')
    data = integrator.load_all_data()
    print(format_enriched_context(data))
